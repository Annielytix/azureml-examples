{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Triton Inference Server locally\n",
    "\n",
    "description: (preview) deploy an image classification model trained on densenet locally via Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install --upgrade tritonclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "successfully downloaded model: densenet_onnx\n",
      "successfully downloaded model: bidaf-9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from src.model_utils import download_triton_models, delete_triton_models\n",
    "\n",
    "prefix = Path(\".\")\n",
    "download_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_path = \"models\"\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=model_path,\n",
    "    model_name=\"densenet-onnx-example\",\n",
    "    tags={\"area\": \"Image classification\", \"type\": \"classification\"},\n",
    "    description=\"Image classification trained on Imagenet Dataset\",\n",
    "    workspace=ws,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "In this case we deploy to the local compute, but for other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/2)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 210B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (5/6)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 210B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for nvcr.io/nvidia/tritonserver:20.11-py3     0.0s\n",
      "\u001b[0m\u001b[34m => [1/2] FROM nvcr.io/nvidia/tritonserver:20.11-py3                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/2] RUN pip install Pillow                                    0.0s\n",
      "\u001b[0m => exporting to image                                                     0.1s\n",
      "\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (6/6) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 210B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for nvcr.io/nvidia/tritonserver:20.11-py3     0.0s\n",
      "\u001b[0m\u001b[34m => [1/2] FROM nvcr.io/nvidia/tritonserver:20.11-py3                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/2] RUN pip install Pillow                                    0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.2s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:1548116fb29dd903915d2b8771ab465e36f7a0e0f3822  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/mytriton                                0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker build . -t mytriton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "645efa955ded22cb0de82822d4f8f05c1fa255047295b44c9bcb103789bff346\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/gopalv/azureml-examples/experimental/deploy-triton/models/triton:/models --env AZUREML_MODEL_DIR=/models mytriton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "import tritonclient.http as tritonhttpclient\n",
    "\n",
    "headers = {\"Content-Type\": \"application/octet-stream\"}\n",
    "\n",
    "test_sample = requests.get(\"https://aka.ms/peacock-pic\", allow_redirects=True).content\n",
    "#img = Image.open(io.BytesIO(test_sample))\n",
    "test=np.array([test_sample], dtype=bytes)\n",
    "test = np.stack(test, axis=0)\n",
    "input = tritonhttpclient.InferInput('img_in_bytes', test.shape, 'BYTES')\n",
    "input.set_data_from_numpy(test)\n",
    "inputs = [input]\n",
    "client = tritonhttpclient.InferenceServerClient(\"localhost:8000\")\n",
    "outputs = [tritonhttpclient.InferRequestedOutput('label')]\n",
    "result = client.infer(model_name='ensemble', inputs=inputs, request_id='1', outputs=outputs)\n",
    "#resp = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
    "#print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice and the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[b'PEACOCK']\n"
     ]
    }
   ],
   "source": [
    "print(result.as_numpy('label'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service.delete()\n",
    "delete_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try changing the deployment configuration to [deploy to Azure Kubernetes Service](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python) for higher availability and better scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "import gevent.ssl\n",
    "\n",
    "import tritonclient.http as tritonhttpclient\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IkZEQTE5MzczRjI4OUQzMDY2ODUzNkNDOUFDRkYyQzg4RjA2MEQwNDMiLCJ0eXAiOiJKV1QifQ.eyJjYW5SZWZyZXNoIjoiRmFsc2UiLCJ3b3Jrc3BhY2VJZCI6ImM5OTdlMWQwLTg4ZmItNDc3ZS1hMDM1LTZiZDkwYmNjMTNlOCIsInRpZCI6IjcyZjk4OGJmLTg2ZjEtNDFhZi05MWFiLTJkN2NkMDExZGI0NyIsIm9pZCI6ImZmZmMxYzY2LTI3NWYtNDkzNS1iYjA0LTcwYTc2MGM4MmZkYSIsImFjdGlvbnMiOiJbXCJNaWNyb3NvZnQuTWFjaGluZUxlYXJuaW5nU2VydmljZXMvd29ya3NwYWNlcy9yZWFkXCIsXCJNaWNyb3NvZnQuTWFjaGluZUxlYXJuaW5nU2VydmljZXMvd29ya3NwYWNlcy9zZXJ2aWNlcy9ha3Mvc2NvcmUvYWN0aW9uXCJdIiwiZW5kcG9pbnROYW1lIjoiZ29wYWx2LWN1c3RvbS1jb250YWluZXIyIiwic2VydmljZUlkIjoiZ29wYWx2LWN1c3RvbS1jb250YWluZXIyIiwiZXhwIjoxNjE4OTI3MDI5LCJpc3MiOiJhenVyZW1sIiwiYXVkIjoiYXp1cmVtbCJ9.N3_0yIKjfkAAtbX_ei2tiWA9LpH-OzIqlelbi31TSU5O24wPuiLoVIYz5wrcx-PnmZpj5YGw8WDwZr9aeJtobqS9IKUr0b6D8tuGrvrsoyH7g7zeSc8PydHWtjPB3vs_ItvXZbzu-OoWznB6DTeo09lBOCySVFyF11IbwUJJ8uMrEOZdwVizELoKLhlTWpKCplMUZK46VQX4Sl7WYWu_YSSGI0QFGJL4spsK0IpF6CiDLDSVb185DtpQyGj2rXsusDKMFg0Vu4KnfdgB-WvvGrBquwxeVMjQcxWVldtoT-bRwnxbedUYfsQJhfW66kDo0YPhY-LZzh4eDNWqJ-nL-w\"}\n",
    "\n",
    "test_sample = requests.get(\"https://aka.ms/peacock-pic\", allow_redirects=True).content\n",
    "#img = Image.open(io.BytesIO(test_sample))\n",
    "test=np.array([test_sample], dtype=bytes)\n",
    "test = np.stack(test, axis=0)\n",
    "input = tritonhttpclient.InferInput('img_in_bytes', test.shape, 'BYTES')\n",
    "input.set_data_from_numpy(test)\n",
    "inputs = [input]\n",
    "outputs = [tritonhttpclient.InferRequestedOutput('label')]\n",
    "client = tritonhttpclient.InferenceServerClient(\"gopalv-custom-container2.westus2-main.inference.ml.azure.com\", ssl=True,\n",
    "ssl_context_factory=gevent.ssl._create_default_https_context)\n",
    "client.is_server_live(headers=headers)\n",
    "result = client.infer(headers=headers, model_name='ensemble', inputs=inputs, request_id='1', outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[b'PEACOCK']\n"
     ]
    }
   ],
   "source": [
    "print(result.as_numpy('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python377jvsc74a57bd053514593536e52de022f29ef618678eddccd581b6db5dc532e9838fb19203af5",
   "display_name": "Python 3.7.7 64-bit ('azureml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "name": "deploy-densenet-local",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}